{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtuomMerkulov/ArtuomMerkulov/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22text_generation_fnet%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "kCd0-vwbHM1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wfSnS5jHXt8",
        "outputId": "66803bf1-c93c-4115-f645-acd6be4bd156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/Numpy/philosophy_dataset.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "hH4vwrbDHTlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(char_to_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neTrwW8fIYIB",
        "outputId": "66873a02-6a1c-4356-e4f1-7d98c45e1df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, '\\x0c': 1, ' ': 2, '!': 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'c': 12, '«': 13, '»': 14, 'а': 15, 'б': 16, 'в': 17, 'г': 18, 'д': 19, 'е': 20, 'ж': 21, 'з': 22, 'и': 23, 'й': 24, 'к': 25, 'л': 26, 'м': 27, 'н': 28, 'о': 29, 'п': 30, 'р': 31, 'с': 32, 'т': 33, 'у': 34, 'ф': 35, 'х': 36, 'ц': 37, 'ч': 38, 'ш': 39, 'щ': 40, 'ъ': 41, 'ы': 42, 'ь': 43, 'э': 44, 'ю': 45, 'я': 46, 'ё': 47, '–': 48, '—': 49, '…': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS1m0RIsI-iU",
        "outputId": "9bcc2d8f-7059-4b44-973a-af50a6765d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  21016\n",
            "Total Vocab:  51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 1000\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwD7AsOfJYk-",
        "outputId": "57940280-84bc-4ebe-c01f-aabcc113d698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  20016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "metadata": {
        "id": "pC7Mhl4iJoNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "cRIs8nk1Jy1Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "yibzp39qKO0f"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0xlfACRTKQ5C",
        "outputId": "e5677b01-f4a7-46e6-d478-c63d1931dcd1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.2961\n",
            "Epoch 1: loss improved from inf to 3.29612, saving model to weights-improvement-01-3.2961.hdf5\n",
            "79/79 [==============================] - 46s 560ms/step - loss: 3.2961\n",
            "Epoch 2/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.2194\n",
            "Epoch 2: loss improved from 3.29612 to 3.21944, saving model to weights-improvement-02-3.2194.hdf5\n",
            "79/79 [==============================] - 46s 584ms/step - loss: 3.2194\n",
            "Epoch 3/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.2135\n",
            "Epoch 3: loss improved from 3.21944 to 3.21348, saving model to weights-improvement-03-3.2135.hdf5\n",
            "79/79 [==============================] - 45s 568ms/step - loss: 3.2135\n",
            "Epoch 4/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.2106\n",
            "Epoch 4: loss improved from 3.21348 to 3.21062, saving model to weights-improvement-04-3.2106.hdf5\n",
            "79/79 [==============================] - 45s 574ms/step - loss: 3.2106\n",
            "Epoch 5/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.2061\n",
            "Epoch 5: loss improved from 3.21062 to 3.20607, saving model to weights-improvement-05-3.2061.hdf5\n",
            "79/79 [==============================] - 45s 575ms/step - loss: 3.2061\n",
            "Epoch 6/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.1990\n",
            "Epoch 6: loss improved from 3.20607 to 3.19902, saving model to weights-improvement-06-3.1990.hdf5\n",
            "79/79 [==============================] - 46s 579ms/step - loss: 3.1990\n",
            "Epoch 7/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.1849\n",
            "Epoch 7: loss improved from 3.19902 to 3.18489, saving model to weights-improvement-07-3.1849.hdf5\n",
            "79/79 [==============================] - 46s 582ms/step - loss: 3.1849\n",
            "Epoch 8/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.1492\n",
            "Epoch 8: loss improved from 3.18489 to 3.14920, saving model to weights-improvement-08-3.1492.hdf5\n",
            "79/79 [==============================] - 46s 584ms/step - loss: 3.1492\n",
            "Epoch 9/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.1174\n",
            "Epoch 9: loss improved from 3.14920 to 3.11740, saving model to weights-improvement-09-3.1174.hdf5\n",
            "79/79 [==============================] - 46s 586ms/step - loss: 3.1174\n",
            "Epoch 10/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0949\n",
            "Epoch 10: loss improved from 3.11740 to 3.09487, saving model to weights-improvement-10-3.0949.hdf5\n",
            "79/79 [==============================] - 46s 585ms/step - loss: 3.0949\n",
            "Epoch 11/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0738\n",
            "Epoch 11: loss improved from 3.09487 to 3.07379, saving model to weights-improvement-11-3.0738.hdf5\n",
            "79/79 [==============================] - 46s 588ms/step - loss: 3.0738\n",
            "Epoch 12/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0592\n",
            "Epoch 12: loss improved from 3.07379 to 3.05924, saving model to weights-improvement-12-3.0592.hdf5\n",
            "79/79 [==============================] - 46s 589ms/step - loss: 3.0592\n",
            "Epoch 13/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0472\n",
            "Epoch 13: loss improved from 3.05924 to 3.04718, saving model to weights-improvement-13-3.0472.hdf5\n",
            "79/79 [==============================] - 47s 590ms/step - loss: 3.0472\n",
            "Epoch 14/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0338\n",
            "Epoch 14: loss improved from 3.04718 to 3.03378, saving model to weights-improvement-14-3.0338.hdf5\n",
            "79/79 [==============================] - 47s 590ms/step - loss: 3.0338\n",
            "Epoch 15/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0158\n",
            "Epoch 15: loss improved from 3.03378 to 3.01575, saving model to weights-improvement-15-3.0158.hdf5\n",
            "79/79 [==============================] - 47s 591ms/step - loss: 3.0158\n",
            "Epoch 16/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0060\n",
            "Epoch 16: loss improved from 3.01575 to 3.00603, saving model to weights-improvement-16-3.0060.hdf5\n",
            "79/79 [==============================] - 47s 599ms/step - loss: 3.0060\n",
            "Epoch 17/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.9861\n",
            "Epoch 17: loss improved from 3.00603 to 2.98612, saving model to weights-improvement-17-2.9861.hdf5\n",
            "79/79 [==============================] - 47s 600ms/step - loss: 2.9861\n",
            "Epoch 18/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.9725\n",
            "Epoch 18: loss improved from 2.98612 to 2.97249, saving model to weights-improvement-18-2.9725.hdf5\n",
            "79/79 [==============================] - 48s 602ms/step - loss: 2.9725\n",
            "Epoch 19/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.9586\n",
            "Epoch 19: loss improved from 2.97249 to 2.95861, saving model to weights-improvement-19-2.9586.hdf5\n",
            "79/79 [==============================] - 48s 609ms/step - loss: 2.9586\n",
            "Epoch 20/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.9336\n",
            "Epoch 20: loss improved from 2.95861 to 2.93362, saving model to weights-improvement-20-2.9336.hdf5\n",
            "79/79 [==============================] - 48s 613ms/step - loss: 2.9336\n",
            "Epoch 21/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.9204\n",
            "Epoch 21: loss improved from 2.93362 to 2.92038, saving model to weights-improvement-21-2.9204.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 2.9204\n",
            "Epoch 22/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.8911\n",
            "Epoch 22: loss improved from 2.92038 to 2.89111, saving model to weights-improvement-22-2.8911.hdf5\n",
            "79/79 [==============================] - 49s 623ms/step - loss: 2.8911\n",
            "Epoch 23/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.8672\n",
            "Epoch 23: loss improved from 2.89111 to 2.86721, saving model to weights-improvement-23-2.8672.hdf5\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 2.8672\n",
            "Epoch 24/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.8343\n",
            "Epoch 24: loss improved from 2.86721 to 2.83427, saving model to weights-improvement-24-2.8343.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 2.8343\n",
            "Epoch 25/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.7998\n",
            "Epoch 25: loss improved from 2.83427 to 2.79976, saving model to weights-improvement-25-2.7998.hdf5\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 2.7998\n",
            "Epoch 26/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.7578\n",
            "Epoch 26: loss improved from 2.79976 to 2.75778, saving model to weights-improvement-26-2.7578.hdf5\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 2.7578\n",
            "Epoch 27/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.7127\n",
            "Epoch 27: loss improved from 2.75778 to 2.71268, saving model to weights-improvement-27-2.7127.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 2.7127\n",
            "Epoch 28/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.6595\n",
            "Epoch 28: loss improved from 2.71268 to 2.65953, saving model to weights-improvement-28-2.6595.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 2.6595\n",
            "Epoch 29/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.5954\n",
            "Epoch 29: loss improved from 2.65953 to 2.59541, saving model to weights-improvement-29-2.5954.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 2.5954\n",
            "Epoch 30/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.5246\n",
            "Epoch 30: loss improved from 2.59541 to 2.52462, saving model to weights-improvement-30-2.5246.hdf5\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 2.5246\n",
            "Epoch 31/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.4464\n",
            "Epoch 31: loss improved from 2.52462 to 2.44637, saving model to weights-improvement-31-2.4464.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 2.4464\n",
            "Epoch 32/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.3649\n",
            "Epoch 32: loss improved from 2.44637 to 2.36490, saving model to weights-improvement-32-2.3649.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 2.3649\n",
            "Epoch 33/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.2597\n",
            "Epoch 33: loss improved from 2.36490 to 2.25972, saving model to weights-improvement-33-2.2597.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 2.2597\n",
            "Epoch 34/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.1638\n",
            "Epoch 34: loss improved from 2.25972 to 2.16383, saving model to weights-improvement-34-2.1638.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 2.1638\n",
            "Epoch 35/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.0497\n",
            "Epoch 35: loss improved from 2.16383 to 2.04966, saving model to weights-improvement-35-2.0497.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 2.0497\n",
            "Epoch 36/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.9239\n",
            "Epoch 36: loss improved from 2.04966 to 1.92389, saving model to weights-improvement-36-1.9239.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 1.9239\n",
            "Epoch 37/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.8032\n",
            "Epoch 37: loss improved from 1.92389 to 1.80316, saving model to weights-improvement-37-1.8032.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 1.8032\n",
            "Epoch 38/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.6775\n",
            "Epoch 38: loss improved from 1.80316 to 1.67746, saving model to weights-improvement-38-1.6775.hdf5\n",
            "79/79 [==============================] - 49s 618ms/step - loss: 1.6775\n",
            "Epoch 39/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.5408\n",
            "Epoch 39: loss improved from 1.67746 to 1.54083, saving model to weights-improvement-39-1.5408.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 1.5408\n",
            "Epoch 40/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.4113\n",
            "Epoch 40: loss improved from 1.54083 to 1.41130, saving model to weights-improvement-40-1.4113.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 1.4113\n",
            "Epoch 41/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.2869\n",
            "Epoch 41: loss improved from 1.41130 to 1.28691, saving model to weights-improvement-41-1.2869.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 1.2869\n",
            "Epoch 42/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.1727\n",
            "Epoch 42: loss improved from 1.28691 to 1.17275, saving model to weights-improvement-42-1.1727.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 1.1727\n",
            "Epoch 43/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.0505\n",
            "Epoch 43: loss improved from 1.17275 to 1.05045, saving model to weights-improvement-43-1.0505.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 1.0505\n",
            "Epoch 44/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.9261\n",
            "Epoch 44: loss improved from 1.05045 to 0.92606, saving model to weights-improvement-44-0.9261.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.9261\n",
            "Epoch 45/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.8352\n",
            "Epoch 45: loss improved from 0.92606 to 0.83516, saving model to weights-improvement-45-0.8352.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.8352\n",
            "Epoch 46/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.7391\n",
            "Epoch 46: loss improved from 0.83516 to 0.73907, saving model to weights-improvement-46-0.7391.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.7391\n",
            "Epoch 47/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.6349\n",
            "Epoch 47: loss improved from 0.73907 to 0.63491, saving model to weights-improvement-47-0.6349.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.6349\n",
            "Epoch 48/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.5677\n",
            "Epoch 48: loss improved from 0.63491 to 0.56774, saving model to weights-improvement-48-0.5677.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.5677\n",
            "Epoch 49/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4831\n",
            "Epoch 49: loss improved from 0.56774 to 0.48312, saving model to weights-improvement-49-0.4831.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.4831\n",
            "Epoch 50/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4098\n",
            "Epoch 50: loss improved from 0.48312 to 0.40981, saving model to weights-improvement-50-0.4098.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.4098\n",
            "Epoch 51/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3486\n",
            "Epoch 51: loss improved from 0.40981 to 0.34865, saving model to weights-improvement-51-0.3486.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.3486\n",
            "Epoch 52/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3203\n",
            "Epoch 52: loss improved from 0.34865 to 0.32031, saving model to weights-improvement-52-0.3203.hdf5\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 0.3203\n",
            "Epoch 53/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2795\n",
            "Epoch 53: loss improved from 0.32031 to 0.27950, saving model to weights-improvement-53-0.2795.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.2795\n",
            "Epoch 54/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2437\n",
            "Epoch 54: loss improved from 0.27950 to 0.24367, saving model to weights-improvement-54-0.2437.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.2437\n",
            "Epoch 55/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2117\n",
            "Epoch 55: loss improved from 0.24367 to 0.21174, saving model to weights-improvement-55-0.2117.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.2117\n",
            "Epoch 56/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1788\n",
            "Epoch 56: loss improved from 0.21174 to 0.17879, saving model to weights-improvement-56-0.1788.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.1788\n",
            "Epoch 57/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1571\n",
            "Epoch 57: loss improved from 0.17879 to 0.15714, saving model to weights-improvement-57-0.1571.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.1571\n",
            "Epoch 58/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1891\n",
            "Epoch 58: loss did not improve from 0.15714\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 0.1891\n",
            "Epoch 59/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1714\n",
            "Epoch 59: loss did not improve from 0.15714\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.1714\n",
            "Epoch 60/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1321\n",
            "Epoch 60: loss improved from 0.15714 to 0.13212, saving model to weights-improvement-60-0.1321.hdf5\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 0.1321\n",
            "Epoch 61/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1056\n",
            "Epoch 61: loss improved from 0.13212 to 0.10558, saving model to weights-improvement-61-0.1056.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.1056\n",
            "Epoch 62/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0940\n",
            "Epoch 62: loss improved from 0.10558 to 0.09398, saving model to weights-improvement-62-0.0940.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.0940\n",
            "Epoch 63/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0937\n",
            "Epoch 63: loss improved from 0.09398 to 0.09368, saving model to weights-improvement-63-0.0937.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.0937\n",
            "Epoch 64/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0859\n",
            "Epoch 64: loss improved from 0.09368 to 0.08588, saving model to weights-improvement-64-0.0859.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.0859\n",
            "Epoch 65/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0805\n",
            "Epoch 65: loss improved from 0.08588 to 0.08045, saving model to weights-improvement-65-0.0805.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.0805\n",
            "Epoch 66/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0905\n",
            "Epoch 66: loss did not improve from 0.08045\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.0905\n",
            "Epoch 67/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0795\n",
            "Epoch 67: loss improved from 0.08045 to 0.07950, saving model to weights-improvement-67-0.0795.hdf5\n",
            "79/79 [==============================] - 49s 620ms/step - loss: 0.0795\n",
            "Epoch 68/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0689\n",
            "Epoch 68: loss improved from 0.07950 to 0.06891, saving model to weights-improvement-68-0.0689.hdf5\n",
            "79/79 [==============================] - 49s 618ms/step - loss: 0.0689\n",
            "Epoch 69/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0705\n",
            "Epoch 69: loss did not improve from 0.06891\n",
            "79/79 [==============================] - 49s 618ms/step - loss: 0.0705\n",
            "Epoch 70/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0668\n",
            "Epoch 70: loss improved from 0.06891 to 0.06677, saving model to weights-improvement-70-0.0668.hdf5\n",
            "79/79 [==============================] - 49s 622ms/step - loss: 0.0668\n",
            "Epoch 71/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0652\n",
            "Epoch 71: loss improved from 0.06677 to 0.06524, saving model to weights-improvement-71-0.0652.hdf5\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.0652\n",
            "Epoch 72/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0591\n",
            "Epoch 72: loss improved from 0.06524 to 0.05911, saving model to weights-improvement-72-0.0591.hdf5\n",
            "79/79 [==============================] - 49s 623ms/step - loss: 0.0591\n",
            "Epoch 73/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0647\n",
            "Epoch 73: loss did not improve from 0.05911\n",
            "79/79 [==============================] - 49s 621ms/step - loss: 0.0647\n",
            "Epoch 74/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.9805\n",
            "Epoch 74: loss did not improve from 0.05911\n",
            "79/79 [==============================] - 49s 619ms/step - loss: 1.9805\n",
            "Epoch 75/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 4.1698\n",
            "Epoch 75: loss did not improve from 0.05911\n",
            "79/79 [==============================] - 48s 612ms/step - loss: 4.1698\n",
            "Epoch 76/100\n",
            "37/79 [=============>................] - ETA: 26s - loss: 3.9931"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f4562124d0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-72-0.0591.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "vNwmIqt_KuoT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "REJh6fSPQKgp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "sFwFSmozQoDX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Read:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts-z6wLcQPTz",
        "outputId": "b10ce6cc-2901-49e3-a1b5-95436e47e457"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read:\n",
            "\"  живет в согласии со всеми, а низкий человек ищет себе подобных.\n",
            "благородный знает о своем превосходстве, но избегает соперничества. он ладит со всеми, но ни с кем не\n",
            "вступает в сговор.\n",
            "благородный муж прям и тверд, но не упрям.\n",
            "благородный муж стремится говорить косноязычно, а действовать искусно.\n",
            "благородный не стремится есть досыта и жить богато. он поспешает в делах, но медлит в речах. общаясь\n",
            "с людьми добродетельными, он исправляет себя. вот о таком человеке можно сказать, что он предан\n",
            "учению.\n",
            "благородный ни от кого не ожидает обмана, но когда его обманывают, он первый замечает это.\n",
            "благородный помогает людям увидеть доброе в себе и не поучает людей видеть в себе дурное. а низкий\n",
            "человек поступает наоборот.\n",
            "благородный превыше всего почитает долг. благородный человек, наделенный отвагой, но не ведающий\n",
            "долга, может стать мятежником. низкий человек, наделенный отвагой, но не ведающий долга, может\n",
            "пуститься в разбой.\n",
            "благородный с достоинством ожидает велений неба. низкий человек с \"\n",
            "уетливо поджидает удачу.\n",
            "благородный стойко переносит беды, а низкий человек в беде распускается.\n",
            "благородный человек знает только долг, низкий человек знает только выгоду.\n",
            "благородный человек предъявляет требования к себе, низкий человек предъявляет требования к другим.\n",
            "благородный, привязанный к домашнему уюту, не достоин зваться таковым.\n",
            "блажен, кто ничего не знает: он не рискует быть непонятым.\n",
            "бросая камень в воду, каждый раз попадаешь в центр круга.\n",
            "будучи вне дома, держите себя так, словно вы принимаете почетных гостей. пользуясь услугами людей,\n",
            "ведите себя так, словно свершаете торжественный обряд. не делайте другим того, чего себе не\n",
            "пожелаете. тогда ни в государстве, ни в семье не будет недовольства.\n",
            "будьте строги к себе и мягки к другим. так вы оградите себя от людской неприязни.\n",
            "быть высоконравственным и значит быть свободным душой. постоянно гневающийся на кого-нибудь,\n",
            "беспрестанно боящийся чего-нибудь и всецело предающийся страстям не может быть свободен душой.\n",
            "кто не мож\n",
            "Done.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}